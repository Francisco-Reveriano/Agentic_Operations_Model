{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-09T16:03:48.276299Z",
     "start_time": "2025-08-09T16:03:41.158140Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from agents import Runner, agent, trace, set_trace_processors, Agent, ModelSettings\n",
    "from weave.integrations.openai_agents.openai_agents import WeaveTracingProcessor\n",
    "import asyncio\n",
    "from typing import Literal, List, Annotated\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, conlist\n",
    "from IPython.display import display, Markdown, HTML, Image\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Upload the file to the File API",
   "id": "7298216829e46c17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T16:08:02.604357Z",
     "start_time": "2025-08-09T16:08:00.834777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def create_file(client, file_path):\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(\n",
    "            file=file_tuple,\n",
    "            purpose=\"assistants\"\n",
    "        )\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(\n",
    "                file=file_content,\n",
    "                purpose=\"assistants\"\n",
    "            )\n",
    "    print(result.id)\n",
    "    return result.id\n",
    "\n",
    "# Replace with your own file path or URL\n",
    "file_id = create_file(client, \"../Data/Knowledge_Base/Workforce_paradigmshift_sid_Kimbho_etal.pdf\")"
   ],
   "id": "8f5818ab4201ea5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-KMjBJPHVCSCdh3F99LDh5Q\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a vector store",
   "id": "3f0335e55990110"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T16:08:20.436848Z",
     "start_time": "2025-08-09T16:08:20.024324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_store = client.vector_stores.create(\n",
    "    name=\"knowledge_base\"\n",
    ")\n",
    "print(vector_store.id)"
   ],
   "id": "6fa2df010f71fb8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_6897727443388191bb580a493335c756\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Add the file to the vector store",
   "id": "99588d24a0581198"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T16:08:28.287646Z",
     "start_time": "2025-08-09T16:08:27.567749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file_id\n",
    ")\n",
    "print(result)"
   ],
   "id": "1b722c0c814f096f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStoreFile(id='file-KMjBJPHVCSCdh3F99LDh5Q', created_at=1754755708, last_error=None, object='vector_store.file', status='in_progress', usage_bytes=0, vector_store_id='vs_6897727443388191bb580a493335c756', attributes={}, chunking_strategy=StaticFileChunkingStrategyObject(static=StaticFileChunkingStrategy(chunk_overlap_tokens=400, max_chunk_size_tokens=800), type='static'))\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check Status",
   "id": "c6579ecad70aab68"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T16:10:13.033908Z",
     "start_time": "2025-08-09T16:10:12.603286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = client.vector_stores.files.list(\n",
    "    vector_store_id=vector_store.id\n",
    ")\n",
    "print(result)"
   ],
   "id": "c32d576915a6f4bf",
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error code: 403 - {'error': \"AI Gateway error: The endpoint 'api.openai.com/v1/vector_stores/vs_6897727443388191bb580a493335c756/files' you requested is not allowed. Make sure the url path is correct and is listed in the documentation (https://docs.prod.ai-gateway.quantumblack.com/models/). If you believe this is an error, please contact the AI Gateway team on #ai_gateway_users channel.\"}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPermissionDeniedError\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m result = \u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvector_stores\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfiles\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlist\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvector_store_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvector_store\u001B[49m\u001B[43m.\u001B[49m\u001B[43mid\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mprint\u001B[39m(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Client_Engagements/Truist_Ops_Model/.venv/lib/python3.11/site-packages/openai/resources/vector_stores/files.py:239\u001B[39m, in \u001B[36mFiles.list\u001B[39m\u001B[34m(self, vector_store_id, after, before, filter, limit, order, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    237\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mExpected a non-empty value for `vector_store_id` but received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvector_store_id\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    238\u001B[39m extra_headers = {\u001B[33m\"\u001B[39m\u001B[33mOpenAI-Beta\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33massistants=v2\u001B[39m\u001B[33m\"\u001B[39m, **(extra_headers \u001B[38;5;129;01mor\u001B[39;00m {})}\n\u001B[32m--> \u001B[39m\u001B[32m239\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_api_list\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    240\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/vector_stores/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mvector_store_id\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m/files\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    241\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpage\u001B[49m\u001B[43m=\u001B[49m\u001B[43mSyncCursorPage\u001B[49m\u001B[43m[\u001B[49m\u001B[43mVectorStoreFile\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    242\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    243\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    244\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    245\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    246\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    247\u001B[39m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mafter\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mafter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbefore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mbefore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfilter\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlimit\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43morder\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfile_list_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mFileListParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mVectorStoreFile\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Client_Engagements/Truist_Ops_Model/.venv/lib/python3.11/site-packages/openai/_base_client.py:1308\u001B[39m, in \u001B[36mSyncAPIClient.get_api_list\u001B[39m\u001B[34m(self, path, model, page, body, options, method)\u001B[39m\n\u001B[32m   1297\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_api_list\u001B[39m(\n\u001B[32m   1298\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1299\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1305\u001B[39m     method: \u001B[38;5;28mstr\u001B[39m = \u001B[33m\"\u001B[39m\u001B[33mget\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   1306\u001B[39m ) -> SyncPageT:\n\u001B[32m   1307\u001B[39m     opts = FinalRequestOptions.construct(method=method, url=path, json_data=body, **options)\n\u001B[32m-> \u001B[39m\u001B[32m1308\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_api_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Client_Engagements/Truist_Ops_Model/.venv/lib/python3.11/site-packages/openai/_base_client.py:1159\u001B[39m, in \u001B[36mSyncAPIClient._request_api_list\u001B[39m\u001B[34m(self, model, page, options)\u001B[39m\n\u001B[32m   1155\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m resp\n\u001B[32m   1157\u001B[39m options.post_parser = _parser\n\u001B[32m-> \u001B[39m\u001B[32m1159\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Documents/Client_Engagements/Truist_Ops_Model/.venv/lib/python3.11/site-packages/openai/_base_client.py:1047\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1044\u001B[39m             err.response.read()\n\u001B[32m   1046\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mPermissionDeniedError\u001B[39m: Error code: 403 - {'error': \"AI Gateway error: The endpoint 'api.openai.com/v1/vector_stores/vs_6897727443388191bb580a493335c756/files' you requested is not allowed. Make sure the url path is correct and is listed in the documentation (https://docs.prod.ai-gateway.quantumblack.com/models/). If you believe this is an error, please contact the AI Gateway team on #ai_gateway_users channel.\"}"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T16:12:05.036356Z",
     "start_time": "2025-08-09T16:11:41.333138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=os.getenv(\"LLM_MODEL\"),\n",
    "    input=\"What is this document about?\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"max_num_results\": 20,\n",
    "        \"vector_store_ids\": [\"vs_6897727443388191bb580a493335c756\"]\n",
    "    }]\n",
    ")\n",
    "print(response)"
   ],
   "id": "76107c8959c4f4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_6897733da01881a090ec11462a20732e00e0578b22e22a38', created_at=1754755901.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_6897733df9ec81a0bbedf62ffb4965ef00e0578b22e22a38', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseFileSearchToolCall(id='fs_689773422fcc81a0b53b116eaa9e982000e0578b22e22a38', queries=['What is this document about?', 'executive summary overview introduction purpose', 'abstract summary key points', 'objective scope background of the document', 'summary of the uploaded document'], status='completed', type='file_search_call', results=None), ResponseReasoningItem(id='rs_68977344f71481a09cdfe9af8a54c75100e0578b22e22a38', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_6897734e12a481a09018378b542f1a2600e0578b22e22a38', content=[ResponseOutputText(annotations=[AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=239, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=452, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=453, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=612, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=824, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=925, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=926, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=1126, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=1127, type='file_citation'), AnnotationFileCitation(file_id='file-KMjBJPHVCSCdh3F99LDh5Q', filename='Workforce_paradigmshift_sid_Kimbho_etal.pdf', index=1337, type='file_citation')], text='It’s a research paper titled “Quantitative Framework for Workforce Optimization in the Age of Large Language Model Agents,” proposing a math-driven way for organizations to plan and staff work as LLM agents are introduced alongside humans .\\n\\nKey points:\\n- Models hybrid human–AI work as a heterogeneous queueing system, treating LLMs as probabilistic “servers,” and derives core equations for effective processing time, staffing levels, and total cost  .\\n- Provides an implementation roadmap (foundation, pilot, scale, optimize) with concrete metrics and go/no-go criteria to manage the transition pragmatically .\\n- Includes a detailed retail banking case study (income verification) showing large efficiency gains and strong financial returns (e.g., ~72% workforce reduction over five years, positive NPV and fast payback) .\\n- Covers sensitivity analysis and Monte Carlo risk modeling to quantify uncertainty and resilience  .\\n- Offers guidance on where LLM automation fits best (high-, medium-, and low-suitability processes) and how to adapt the framework across functions, with notes on regulatory and ethical constraints  .\\n\\nOverall, it’s a practical, operations-research framework for C‑suite and operations leaders to size teams, budget, and phase AI adoption with quantifiable ROI and risk controls in LLM-augmented environments .', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_6897727443388191bb580a493335c756'], filters=None, max_num_results=20, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=19648, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=1370, output_tokens_details=OutputTokensDetails(reasoning_tokens=960), total_tokens=21018), user=None, store=False)\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
